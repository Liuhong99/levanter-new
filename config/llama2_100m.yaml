data: !include data/dolma_source.yaml
model:
  type: llama
  hidden_dim: 768
  intermediate_dim: 2304
  num_heads: 12
  num_layers: 12
  seq_len: 8192
  gradient_checkpointing: True

trainer:
  wandb:
    project: "llama-100m"
    tags: ["dolma", "llama_100m"]
  mp: p=f32,c=bfloat16
  train_batch_size: 1024
  num_train_steps: 1000_000
  steps_per_eval: 500
optimizer:
  learning_rate: 3E-3
  weight_decay: 0.1
  min_lr_ratio: 0.1
